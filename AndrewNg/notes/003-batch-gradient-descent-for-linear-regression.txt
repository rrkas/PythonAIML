linear regression:
    h(x) = @[0] + @[1].x                                                   ...(1)
    J(@[0], @[1]) = (1/2m) summation( i=1..m, (h(x[i]) - y[i])^2 )                  ...(2)

gradient convergence:
    repeat until convergence {
        @[j] := @[j] - a ( del(J(@[0], @[1]) / del(@[j]) )       for j=0 and j=1    ...(3)
    }

putting (1) in (2) and then (2) in (3),
    @[0] = (1/m) summation (i=1..m, (h(x[i]) - y[i]))
    @[1] = (1/m) summation (i=1..m, (h(x[i]) - y[i]).x[i] )


new gradient convergence:
    repeat until convergence {
        @[0] := @[0] - (a/m) summation (i=1..m, (h(x[i]) - y[i]))
        @[1] := @[1] - (a/m) summation (i=1..m, (h(x[i]) - y[i]).x[i] )
    }
    update @[0] and @[1] simultaneously

